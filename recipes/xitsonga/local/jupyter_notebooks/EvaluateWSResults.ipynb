{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paramaters and paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "experiment_dir = '/scratch/owb/Downloads/amdtk/recipes/xitsonga_mllt_fmllr_lda/ploop_mfcc_c1_T100_sil0_s3_g2_a3_b3'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unigram/Bigram results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unigram_labels_nmi_file = os.path.join(experiment_dir, 'unigram_labels_nmi/scores')\n",
    "unigram_lattices_nmi_file = os.path.join(experiment_dir, 'unigram_lattices_nmi/scores')\n",
    "bigram_labels_nmi_file = os.path.join(experiment_dir, 'bigram_labels_nmi/scores')\n",
    "bigram_lattices_nmi_file = os.path.join(experiment_dir, 'bigram_lattices_nmi/scores')\n",
    "unigram_states_abx_file = os.path.join(experiment_dir, 'unigram_eval1/unigram_gmm_posts_txt/results.txt')\n",
    "unigram_units_abx_file = os.path.join(experiment_dir, 'unigram_eval1/unigram_unit_gmm_posts_txt/results.txt')\n",
    "bigram_states_abx_file = os.path.join(experiment_dir, 'bigram_eval1/bigram_gmm_posts_txt/results.txt')\n",
    "bigram_units_abx_file = os.path.join(experiment_dir, 'bigram_eval1/bigram_unit_gmm_posts_txt/results.txt')\n",
    "baseline_header = ['Experiment',\n",
    "                   'NMI (labels)', 'NMI (lattices)',\n",
    "                   'State ABX (within)', 'State ABX (across)',\n",
    "                   'Unit ABX (within)', 'Unit ABX (across)']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word segmentation and retraining results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#labels_nmi_path = 'bigram_ws_1best/*/TimedSentences_Iter_150_1best_nmi/scores'\n",
    "#lattices_nmi_path = 'bigram_ws_1best/*/TimedSentences_Iter_150_1best_va_nmi/scores'\n",
    "#posts_abx_path = 'bigram_ws_eval1/bigram_ws_1best/*/TimedSentences_Iter_150_1best_posts_txt/results.txt'\n",
    "eval2_base = 'bigram_ws_eval2/bigram_ws/*/*/TimedSentences_Iter_150'\n",
    "eval2_measures = ['boundary', 'group', 'matching', 'nlp', 'token_type']\n",
    "result_header = ['Experiment',\n",
    "                 #'NMI (labels)', 'NMI (lattice)',\n",
    "                 # 'ABX (within)', 'ABX (across)',\n",
    "                 'Boundary precision (total)', 'Boundary recall (total)', 'Boundary fscore (total)',\n",
    "                 'Boundary precision (within)', 'Boundary recall (within)', 'Boundary fscore (within)',\n",
    "                 'Grouping precision (total)', 'Grouping recall (total)', 'Grouping fscore (total)',\n",
    "                 'Grouping precision (within)', 'Grouping recall (within)', 'Grouping fscore (within)',\n",
    "                 'Matching precision (total)', 'Matching recall (total)', 'Matching fscore (total)',\n",
    "                 'Matching precision (within)', 'Matching recall (within)', 'Matching fscore (within)',\n",
    "                 'NED (total)', 'Coverage (total)',\n",
    "                 'NED (within)', 'Coverage (within)',\n",
    "                 'Token precision (total)', 'Token recall (total)', 'Token fscore (total)',\n",
    "                 'Type precision (total)', 'Type recall (total)', 'Type fscore (total)',\n",
    "                 'Token precision (within)', 'Token recall (within)', 'Token fscore (within)',\n",
    "                 'Type precision (within)', 'Type recall (within)', 'Type fscore (within)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#labels_nmi_files = glob.glob(os.path.join(experiment_dir, labels_nmi_path))\n",
    "#lattices_nmi_files = glob.glob(os.path.join(experiment_dir, lattices_nmi_path))\n",
    "#posts_abx_files = glob.glob(os.path.join(experiment_dir, posts_abx_path))\n",
    "eval2_directories = glob.glob(os.path.join(experiment_dir, eval2_base))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read NMI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_nmi(file):\n",
    "    experiment = file.split('/')[-3]\n",
    "    with open(file) as fp:\n",
    "        lines = fp.readlines()\n",
    "        for line in lines:\n",
    "            line_split = line.split()\n",
    "            if line_split[0] == 'I(X;Y)/(H(X)):':\n",
    "                return (experiment, float(line_split[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "baseline_nmi = {'unigram': [read_nmi(unigram_labels_nmi_file)[1], read_nmi(unigram_lattices_nmi_file)[1]],\n",
    "                'bigram': [read_nmi(bigram_labels_nmi_file)[1], read_nmi(bigram_lattices_nmi_file)[1]]}\n",
    "#labels_nmi = {experiment: result for experiment, result in [read_nmi(file) for file in labels_nmi_files]}\n",
    "#lattices_nmi = {experiment: result for experiment, result in [read_nmi(file) for file in lattices_nmi_files]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read ABX scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_abx_scores(file):\n",
    "    output_measures = {'within_talkers', 'across_talkers'}\n",
    "    experiment = file.split('/')[-3]\n",
    "    try:\n",
    "        with open(file) as fp:\n",
    "            lines = fp.readlines()\n",
    "            result = [float(line_split[1]) for line_split in [line.split() for line in lines]\n",
    "                     if len(line_split) > 1 and len(line_split[0]) > 13 and line_split[0][-15:-1] in output_measures]\n",
    "            return (experiment, result)\n",
    "    except IOError:\n",
    "            return (experiment, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "baseline_abx = {'unigram': read_abx_scores(unigram_states_abx_file)[1] + read_abx_scores(unigram_units_abx_file)[1],\n",
    "                'bigram': read_abx_scores(bigram_states_abx_file)[1] + read_abx_scores(bigram_units_abx_file)[1]}\n",
    "#posts_abx = {experiment: result for experiment, result in [read_abx_scores(file) for file in posts_abx_files]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read eval2 measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_eval2_measure(file):\n",
    "    output_measures = {'precision', 'recall', 'fscore', 'NED', 'coverage'}\n",
    "    experiment = '_'.join(file.split('/')[-4:-2])\n",
    "    try:\n",
    "        with open(file) as fp:\n",
    "            lines = fp.readlines()\n",
    "            result = [float(line_split[1]) for line_split in [line.split() for line in lines]\n",
    "                     if len(line_split) > 1 and line_split[0] in output_measures]\n",
    "            return (experiment, result)\n",
    "    except IOError:\n",
    "            return (experiment, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "eval2_results =  {measure: {experiment: result for experiment, result in \n",
    "                            [read_eval2_measure(os.path.join(directory, measure)) for directory in eval2_directories]}\n",
    "                  for measure in eval2_measures}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine and output results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "results = dict()\n",
    "#for experiment, result in labels_nmi.items():\n",
    "#    results[experiment] = [result]\n",
    "#for experiment, result in lattices_nmi.items():\n",
    "#    results[experiment].append(result)\n",
    "#for experiment, result in posts_abx.items():\n",
    "#    results[experiment].extend(result)\n",
    "for measure in eval2_measures:\n",
    "    for experiment, result in eval2_results[measure].items():\n",
    "        if len(result) == 0:\n",
    "            if measure == 'nlp':\n",
    "                result = [1, 0]*2\n",
    "            else:\n",
    "                result = [0]*6\n",
    "        try:\n",
    "            results[experiment] += result\n",
    "        except KeyError:\n",
    "            results[experiment] = result\n",
    "                \n",
    "baseline_results = {experiment: baseline_nmi[experiment] + baseline_abx[experiment] for experiment in baseline_nmi}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(experiment_dir, 'results.csv'), 'w') as fid:\n",
    "    string_format = '{}' + ';{}'*(len(result_header) - 1) + '\\n'\n",
    "    fid.write(string_format.format(*result_header))\n",
    "    for experiment, result in results.items():\n",
    "        fid.write(string_format.format(experiment, *result))\n",
    "\n",
    "    fid.write('\\n')\n",
    "    \n",
    "    string_format = '{}' + ';{}'*(len(baseline_header) - 1) + '\\n'\n",
    "    fid.write(string_format.format(*baseline_header))\n",
    "    for experiment, result in baseline_results.items():\n",
    "        fid.write(string_format.format(experiment, *result))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
