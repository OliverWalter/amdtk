#!/usr/bin/env python

'''Score amdtk cluster labels against reference labels.'''

import argparse
import os
import numpy as np
from amdtk.core.internal_io import readMlf

class OffsetFromSegmentFile:
    def __init__(self, segments_file):
        with open(segments_file) as fp:
            self.segments_offset = dict()
            for line in fp:
                file, start, end = line.split()
                self.segments_offset[file] = float(start)

    def __call__(self, file):
        return self.segments_offset[os.path.splitext(file)[0]]


def __is_start_or_end_field(field):
    return field.isdigit()

def __is_score_field(field):
    try:
        float(field)
    except ValueError:
        return False
    return True

def probability_matrix(counts, alpha=1):
    c = np.array(counts + alpha, dtype=float)
    return (c.T/c.sum(axis=1)).T

def get_labels_clusters(T_ref, T_new, ignore_sil):
    counts = {}
    t_counts = {}
    label_set = set()
    cluster_set = set()
    for utt in T_ref.keys():
        labels = T_ref[utt]
        clusters = T_new[utt]
        for t in labels:
            label, start, stop, _, _ = t
            label_set.add(label)
        for t in clusters:
            cluster, start, stop, _, _ = t
            if not ignore_sil or cluster != 'sil':
                cluster_set.add(cluster)

    return list(label_set), list(cluster_set)

def align(T_ref, T_new, labels, clusters, offset, ignore_sil):
    counts_labels = np.zeros(len(labels))
    counts_clusters = np.zeros(len(clusters))
    counts = np.zeros((len(clusters), len(labels))) + 1e-64
    for utt in T_ref.keys():
        data_ref = T_ref[utt]
        data_new = T_new[utt]
        label_offset = 0
        if offset is not None:
            label_offset = offset(utt)*100

        mu = []
        for t in data_ref:
            label, start, stop, _, _ = t
            mu.append(start+0.5*(stop-start))
            idx = labels.index(label)
            counts_labels[idx] += 1
        mu = np.asarray(mu)

        for t in data_new:
            cluster, start, stop, _, _ = t
            if not ignore_sil or cluster != 'sil':
                idx = clusters.index(cluster)
                counts_clusters[idx] += 1
                x = start + 0.5*(stop-start) + label_offset
                closest_label = ((x-mu)**2).argmin()
                i = clusters.index(cluster)
                j = labels.index(data_ref[closest_label][0])
                counts[i,j] += 1

    return counts, counts_labels, counts_clusters

def main():
    # Command line parsing.
    #-----------------------------------------------------------------------
    parser = argparse.ArgumentParser(description=__doc__)
    parser.add_argument('--ignore_sil', action='store_true',
                        help='ignore "sil" tokens in proposed MLF')
    parser.add_argument('-s', '--segments_file',
                        help='Segments file with timing offset information for result mlf')
    parser.add_argument('ref_mlf', help='reference MLF')
    parser.add_argument('new_mlf', help='proposed MLF')
    args = parser.parse_args()

    # Load the MLFs.
    #-----------------------------------------------------------------------
    T_ref = readMlf(args.ref_mlf)
    T_new = readMlf(args.new_mlf)

    # parse offset file
    #-----------------------------------------------------------------------
    offset = None
    if args.segments_file is not None:
        offset = OffsetFromSegmentFile(args.segments_file)

    # Extract the reference and cluster labels.
    #-----------------------------------------------------------------------
    labels, clusters = get_labels_clusters(T_ref, T_new, args.ignore_sil)
    
    # Map each cluster in the new transcription to the closest (in time)
    # reference label.
    #-----------------------------------------------------------------------
    counts, counts_labels, counts_clusters = align(T_ref, T_new, labels,
                                                   clusters, offset,
                                                   args.ignore_sil)
    print('# counts:', counts.sum())

    # Estimate the conditional distribution of the reference label given the 
    # cluster.
    #-----------------------------------------------------------------------
    p_X_given_Y = probability_matrix(counts, alpha=1e-12)

    # Estimate the marginal distribution of the reference cluster labels. 
    #-----------------------------------------------------------------------
    p_X = np.asarray(counts.sum(axis=0), dtype=float) 
    p_X /= p_X.sum()
    p_Y = np.asarray(counts.sum(axis=1), dtype=float) 
    p_Y /= p_Y.sum()

    # Evaluate the conditional and marginal entropy.
    #-----------------------------------------------------------------------
    H_X_given_Y = -(p_Y.dot((p_X_given_Y*np.log2(p_X_given_Y)).sum(axis=1)))
    H_X = -p_X.dot(np.log2(p_X))
    H_Y = -p_Y.dot(np.log2(p_Y))
    
    # Evaluate the mutual information between reference labels and clusters.
    #-----------------------------------------------------------------------
    I_XY = H_X - H_X_given_Y

    print('# refs units:', len(labels))
    print('# proposed units:', len(clusters))
    print('H(X):', H_X)
    print('H(Y):', H_Y)
    print('H(X|Y):', H_X_given_Y)
    print('2*I(X;Y)/(H(X) + H(Y)):', 100*2*I_XY/(H_X + H_Y), '%')
    print('I(X;Y)/(H(X)):', 100*I_XY/(H_X ), '%')
    print('I(X;Y):',I_XY)

if __name__ == '__main__':
    main()
else:
    raise ImportError('this script cannot be imported')
